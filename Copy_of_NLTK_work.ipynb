{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUFIgz79RD9bjZo/UWO6fq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anietim/3MTT-GOMYCODE-AltSchool-Africa/blob/main/Copy_of_NLTK_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "C7sCP_3YQIwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002fe3a6-56bf-4121-ba09-a0511a537c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown.words()"
      ],
      "metadata": {
        "id": "J9JGKxHSQgG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bff66a5-8dc2-4dd9-8a41-0a9d4be3ad7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STqowgIROokC",
        "outputId": "9618113c-d1ef-4441-fc1e-730aefa7f2e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tikennization example\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text='this is some random text we are using to demostrate the implimentation of tokenization. please follow through'\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7FL6sEWOooq",
        "outputId": "5b331dbe-c0a1-4904-f39e-67feeed93b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this is some random text we are using to demostrate the implimentation of tokenization.', 'please follow through']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenization example\n",
        "from nltk.tokenize import word_tokenize\n",
        "text='this is an example of word tokenization'\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDRODWrjOouJ",
        "outputId": "7afcf55d-5ce7-4c7e-8592-6fc240aa3389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'an', 'example', 'of', 'word', 'tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVE3XHU_X4__",
        "outputId": "4f1b1fbb-ae9d-4362-c4c4-8d7ea730fdb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop words example\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IsEVhhVOoxy",
        "outputId": "075d0a3e-8587-4234-bb3e-3d9b0612c41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "set(stopwords.words('english'))\n",
        "ps=PorterStemmer()\n",
        "text='They were cooking all evening for the queen of wakander'\n",
        "worfs=word_tokenize(text)\n",
        "for word in worfs:\n",
        "  print(ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNhqA_KDOo14",
        "outputId": "bba5cb5c-38a9-40b9-bc12-0c971a5ede4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "they\n",
            "were\n",
            "cook\n",
            "all\n",
            "even\n",
            "for\n",
            "the\n",
            "queen\n",
            "of\n",
            "wakand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOPa9MSkgdx8",
        "outputId": "66b56762-a4df-4f84-a30b-e70ca08100fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(\"drinks:\", lemmatizer. lemmatize(\"drinks\"))\n",
        "print(\"corpora:\", lemmatizer. lemmatize(\"copora\"))\n",
        "print(\"sunsets:\", lemmatizer. lemmatize(\"sunsets\"))\n",
        "print(\"compueters:\", lemmatizer. lemmatize(\"computers\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt3MWOpAOo6A",
        "outputId": "a24f9c46-cf5c-4c0f-c4e8-2e5b1017d4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drinks: drink\n",
            "corpora: copora\n",
            "sunsets: sunset\n",
            "compueters: computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tag\n",
        "from nltk.tag import DefaultTagger\n",
        "# defining the tag\n",
        "tagging = DefaultTagger('NN')\n",
        "# Tagging\n",
        "tagging.tag([\"hello, Geek\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fnH2HKdOo-O",
        "outputId": "35120e01-96c6-4143-a962-ca21b94723ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello, Geek', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF6MW2X-G2C2",
        "outputId": "7525a36c-29f5-45cc-c350-e3ca948577af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQBwxdk8MBbb",
        "outputId": "2753a73e-c5ba-44e3-cf2f-27f1467fbe89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chV7cmGTMmnL",
        "outputId": "811389a3-6bd4-4a6b-fca1-f657caa9affe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBdly2vbM9JD",
        "outputId": "b89dcb0c-9106-4fc9-d0b6-f8b59288823b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "text ='''By Mark Potts/Washington Post Writer\n",
        "BALTIMORE, March 22--A computer \"Hacker\" who was trying to help others steal\n",
        "electronic passwords guarding large corporate computer systems around'\n",
        "the country today pleaded guilty to wire fraud in a continuing goovernment\n",
        "crackdown on computer crime.\n",
        "\n",
        "Federal prosecutors recommended that Leonard Rose Jr., 32 of Milldetown, Md.,\n",
        "be sent to prison for one year and one day, which would be one of thenstiffest\n",
        "sentences imposd to date for computer crime. Sentencing is scheduled for May\n",
        "before U.S. Distrist Judge J. Fredrick Motz.\n",
        "'''\n",
        "\n",
        "# # tokennize text\n",
        "tokenize_text = word_tokenize(text)\n",
        "tag_sentences=nltk.pos_tag(tokenize_text)\n",
        "ner_chuncked_sentences=nltk.ne_chunk(tag_sentences)\n",
        "\n",
        "# extract all NEs\n",
        "ne=[]\n",
        "\n",
        "for tagged_tree in ner_chuncked_sentences:\n",
        "  if hasattr(tagged_tree, 'label'):\n",
        "    entity_name=' '.join(c[0] for c in tagged_tree.leaves())\n",
        "    entity_type=tagged_tree.label()\n",
        "    ne.append((entity_name,entity_type))\n",
        "    print(ne)"
      ],
      "metadata": {
        "id": "T7-9fx-bQgbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63165756-4aa3-4392-99f2-29a6bc42caa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Mark Potts/Washington Post Writer BALTIMORE', 'PERSON')]\n",
            "[('Mark Potts/Washington Post Writer BALTIMORE', 'PERSON'), ('Leonard Rose Jr.', 'PERSON')]\n",
            "[('Mark Potts/Washington Post Writer BALTIMORE', 'PERSON'), ('Leonard Rose Jr.', 'PERSON'), ('Milldetown', 'GPE')]\n",
            "[('Mark Potts/Washington Post Writer BALTIMORE', 'PERSON'), ('Leonard Rose Jr.', 'PERSON'), ('Milldetown', 'GPE'), ('U.S.', 'GPE')]\n",
            "[('Mark Potts/Washington Post Writer BALTIMORE', 'PERSON'), ('Leonard Rose Jr.', 'PERSON'), ('Milldetown', 'GPE'), ('U.S.', 'GPE'), ('Fredrick Motz', 'PERSON')]\n"
          ]
        }
      ]
    }
  ]
}